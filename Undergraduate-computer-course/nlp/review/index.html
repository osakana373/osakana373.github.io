<!-- build time:Thu Nov 09 2023 10:47:53 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="OSAKANA" href="https://osakana373.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="OSAKANA" href="https://osakana373.github.io/atom.xml"><link rel="alternate" type="application/json" title="OSAKANA" href="https://osakana373.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="计算机本科课程"><link rel="canonical" href="https://osakana373.github.io/Undergraduate-computer-course/nlp/review/"><title>review - 自然语言处理 - 人工智能 - 计算机本科课程 | OSAKANA = OSAKANA = 一緒に夢を見よう</title><meta name="generator" content="Hexo 5.4.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">review</h1><div class="meta"><span class="item" title="创建时间：2022-01-03 22:25:34"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2022-01-03T22:25:34+08:00">2022-01-03</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>4.6k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>4 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">OSAKANA</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://s2.loli.net/2023/10/17/41sHVDqpGXJ9fId.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2023/10/17/o4t5VECPQDBbzOS.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2023/10/17/NaQTivZ1XtCLSEd.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2023/10/17/M9zNVT67rhDRkpI.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2023/10/17/9DknJibPe7LXUuY.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2023/10/17/zrvyJBQLYwXPIVa.png"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/Undergraduate-computer-course/" itemprop="item" rel="index" title="分类于 计算机本科课程"><span itemprop="name">计算机本科课程</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/Undergraduate-computer-course/AI/" itemprop="item" rel="index" title="分类于 人工智能"><span itemprop="name">人工智能</span></a><meta itemprop="position" content="2"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/Undergraduate-computer-course/AI/nlp/" itemprop="item" rel="index" title="分类于 自然语言处理"><span itemprop="name">自然语言处理</span></a><meta itemprop="position" content="3"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://osakana373.github.io/Undergraduate-computer-course/nlp/review/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/touxiang.jpg"><meta itemprop="name" content="おうじせん"><meta itemprop="description" content="一緒に夢を見よう, 誰よりスキだから"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="OSAKANA"></span><div class="body md" itemprop="articleBody"><h1 id="分词"><a class="anchor" href="#分词">#</a> 分词</h1><p>为什么需要分词？</p><ul><li>在自然语言处理领域，信息处理的最小单位一般是词</li><li>在中文里，词和词之间没有天然的分隔，比如空格，因此在进行其他任务之前，通常需要分词</li></ul><h2 id="分词算法"><a class="anchor" href="#分词算法">#</a> 分词算法</h2><ul><li>查词典法<ul><li>最大匹配法</li><li>最大概率法</li></ul></li><li>序列标注法</li><li>N - 最短路径法</li></ul><h2 id="新词发现"><a class="anchor" href="#新词发现">#</a> 新词发现</h2><p>为什么需要新词发现？</p><ul><li>基于词典的分词方法对于没在词典的词（新词）没办法处理</li><li>新词也称为未登录词</li><li>人名、地名、机构名、品牌名、专业名词、缩略语、网络新词</li></ul><p>新词发现的思路：</p><ul><li>根据语料（文本数据）中词的一些特征，将语料中可能的词提取出来</li><li>再把所有提取出来的词和词典里的词进行比较</li><li>不在词典里的词就是新词</li></ul><p>新词特征：频率、内部凝固度（互信息）、外部自由度（信息熵）</p><h1 id="词向量"><a class="anchor" href="#词向量">#</a> 词向量</h1><h2 id="名词解释"><a class="anchor" href="#名词解释">#</a> 名词解释</h2><p>分词：将句子、段落、文章这种长文本，分解为以字词为单位的数据结构，方便后续的处理分析工作</p><p>词向量：用来表示词的向量，也可被认为是词的特征向量或表征。词向量本质是将一些低维、离散、不带任何意义的序号映射成带有特定任务性质的高维特征。</p><h3 id="语言模型"><a class="anchor" href="#语言模型">#</a> 语言模型</h3><p>语言模型：计算一个词出现的概率，极大似然估计；计算一段文字出现的概率</p><p>n 元语法模型：一个词出现的概率只和它前面 n-1 个词相关</p><p>数据平滑的基本思想：调整概率值，使零概率增值，使非零概率下调，改进模型的整体性能（加 1 法，Good-Turing 平滑）</p><p>Good-Turing 平滑：将已经出现的词的概率进行打折，打折后分出一部分概率给没有出现的词</p><h3 id="神经语言模型"><a class="anchor" href="#神经语言模型">#</a> 神经语言模型</h3><p>独热向量：每个词对应一维；向量的总维数为词表大小</p><p>例如：Wuhan University is located in Hubei，则 University 的 one-hot 为：0 1 0 0 0 0 0</p><p>输入层：n-1 个词的词向量串接</p><p>隐层：y = f (Wx+b)</p><ul><li>x 输入向量</li><li>W 参数矩阵</li><li>b 偏置向量</li><li>f 激励函数（sigmoid, tanh）</li><li>y 输出向量</li></ul><p>输出层：softmax</p><p><img data-src="1.png" alt></p><h2 id="word2vec"><a class="anchor" href="#word2vec">#</a> Word2Vec</h2><p>Word embedding: 把词表示成一个固定维数的向量</p><p>向量的每一维都是参数，是从大规模的文本中训练出来</p><ul><li>Word2Vec 的参数一个是 C，一个是 W，其中 C 就是词向量的查找表，其中每一行（或每一列）就是我们常说的词向量</li><li>模型训练好后，我们可以把 C 保存下来</li><li>Word2Vec 的主要计算量在输出层</li><li>因为词表 | V | 很大，所以 softmax 的计算量很大</li><li>Word2Vec 提出两种加快训练速度的方式，思想都是减少 softmax 计算</li></ul><p>Word2Vec 和 Glove 均可以训练词向量</p><h1 id="文本分类"><a class="anchor" href="#文本分类">#</a> 文本分类</h1><h2 id="文本分类任务介绍"><a class="anchor" href="#文本分类任务介绍">#</a> 文本分类任务介绍</h2><p>任务定义：输入一个句子或一段文本，输出句子或者文本的类别（情感分类、关系分类）。</p><p>常用方法：</p><ul><li>统计机器学习：支持向量机 SVM、最大熵模型</li><li>深度学习：卷积神经网络 CNN、循环神经网络 RNN</li></ul><h2 id="基于cnn的文本分类"><a class="anchor" href="#基于cnn的文本分类">#</a> 基于 CNN 的文本分类</h2><p>卷积：用卷积核在序列上进行重复的类似的计算。</p><p>卷积目的：单个词表达的意义是有限的，让词和它的上下文互相感知，从而丰富词的语义</p><p>池化：将多个向量变成一个向量</p><p>基于 CNN 的文本分类总结：</p><ul><li>利用卷积学习词和词的上下文特征</li><li>利用池化将一个变长的句子转化为一个定长的特征向量</li><li>特征向量用于输出层做文本分类<ul><li>softmax</li></ul></li></ul><p>TextCNN：定义了 6 个卷积核（4x5,3x5,2x5 各两个）</p><p>TextCNN 总结：</p><ul><li>CNN 可以有多个核，每个核的窗口大小不一样</li><li>便于捕捉 n 元特征（2 元，3 元，4 元，...）</li><li>多个核产生的特征，通过串接组合在一起，生成特征向量</li><li>特征向量用于输出层做文本分类</li></ul><h2 id="基于cnn的关系分类"><a class="anchor" href="#基于cnn的关系分类">#</a> 基于 CNN 的关系分类</h2><h3 id="关系分类的任务介绍"><a class="anchor" href="#关系分类的任务介绍">#</a> 关系分类的任务介绍</h3><p>给定两个实体 (人名、地名、组织结构名等等)，判断两个实体之间的关系（分类），例如：小李出生在中国（出生在就是关系）</p><p>基于 CNN：</p><ul><li>句子</li><li>词表示</li><li>卷积</li><li>池化</li><li>输出层</li></ul><p>加入 Attention 机制后：</p><ul><li>句子</li><li>词表示</li><li>注意力 Attention</li><li>卷积</li><li>Attention</li><li>输出层</li></ul><p>CNN 可以和其他神经网络，如前馈神经网络、循环神经网络、Attention 网络进行组合使用</p><p>分类模型的要素:</p><ul><li>输入</li><li>隐层</li><li>输出层</li><li>损失函数</li></ul><h3 id="注意力机制"><a class="anchor" href="#注意力机制">#</a> 注意力机制</h3><p>目的：学习每个词对于任务的贡献率</p><ul><li>输入：一个向量序列</li><li>“something” attention on 向量序列 ，计算 attention score</li><li>通过 softmax 归一化 attention score</li></ul><p>Attention score 的用途：</p><ul><li>可以用来衡量每个词的贡献</li><li>可以用来进行加权平均，得到特征</li></ul><h1 id="词性标注"><a class="anchor" href="#词性标注">#</a> 词性标注</h1><h2 id="任务介绍"><a class="anchor" href="#任务介绍">#</a> 任务介绍</h2><p>词性标注，就是给每个词一个词性</p><p>HMM（隐马尔可夫模型）可用于词性标注：</p><ul><li>训练数据</li><li>计算马尔可夫模型的参数<ul><li>利用极大似然估计</li></ul></li><li>预测一个句子中，每个词的词性<ul><li>Viterbi 算法（词性预测）</li></ul></li></ul><p>HMM 参数：</p><ul><li>隐状态</li><li>观察值</li><li>转移概率</li><li>发射概率</li></ul><h1 id="实体识别"><a class="anchor" href="#实体识别">#</a> 实体识别</h1><h2 id="任务介绍-2"><a class="anchor" href="#任务介绍-2">#</a> 任务介绍</h2><p>实体识别（NER）定义：识别出文本中实体（人名、机构名、地名、时间、日期等）</p><p>两个子任务：</p><ul><li>实体边界识别</li><li>确定实体类别</li></ul><p>方法：把实体识别看成序列标注任务，为句子中的每一个词打标签</p><p>基于 Bi-directional LSTM CRF 的实体识别：</p><ul><li>词向量层</li><li>双向 LSTM 层</li><li>CRF 层</li></ul><h2 id="循环神经网络"><a class="anchor" href="#循环神经网络">#</a> 循环神经网络</h2><p>变体：LSTM、GRU</p><h3 id="lstm"><a class="anchor" href="#lstm">#</a> LSTM</h3><p>LSTM：全名为 Long Shoort Term 网络，是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力。</p><p>LSTM 的核心概念在于细胞状态以及 “门” 结构。</p><p>细胞状态相当于信息传输的路径，让信息能在序列连中传递下去。理论上讲，细胞状态能够将序列处理过程中的相关信息一直传递下去。因此，即使是较早时间步长的信息也能携带到较后时间步长的细胞中来，这克服了短时记忆的影响。</p><p>信息的添加和移除我们通过 “门” 结构来实现，“门” 结构在训练过程中会去学习该保存或遗忘哪些信息。</p><p>遗忘门：决定应丢弃或保留哪些信息。来自前一个隐藏状态的信息和当前输入的信息同时传递到 sigmoid 函数中去，输出值介于 0 和 1 之间，越接近 0 意味着越应该丢弃，越接近 1 意味着越应该保留。</p><p>输入门：用于更新细胞状态。首先将前一层隐藏状态的信息和当前输入的信息传递到 sigmoid 函数中去。将值调整到 01 之间来决定要更新哪些信息。0 表示不重要，1 表示重要。其次还要将前一层隐藏状态的信息和当前输入的信息传递到 tanh 函数中去，创造一个新的侯选值向量。最后将 sigmoid 的输出值与 tanh 的输出值相乘，sigmoid 的输出值将决定 tanh 的输出值中哪些信息是重要且需要保留下来的。</p><p>细胞状态：更新旧细胞状态。我们把旧状态与遗忘门输出值相乘，丢弃掉我们确定需要丢弃的信息。接着加上输入门输出的值（sigmoid 的输出值与 tanh 的输出值相乘），这就是新的候选值，根据我们决定更新每个状态的程度进行变化。</p><p>输出门：用来确定下一个隐藏状态的值，隐藏状态包含了先前输入的信息。首先，我们将前一个隐藏状态和当前输入传递到 sigmoid 函数中，然后将新得到的细胞状态传递给 tanh 函数。最后将 tanh 的输出与 sigmoid 的输出相乘，以确定隐藏状态应携带的信息。再将隐藏状态作为当前细胞的输出，把新的细胞状态和新的隐藏状态传递到下一个时间步长中去。</p><p><img data-src="LSTM.png" alt></p><p>LSTM 和 GRU 的比较:</p><ul><li>LSTM：输入门，输出门，遗忘门，cell state</li><li>GRU：更新门，重置门，输出门</li></ul><p>循环神经网络可用于特征抽取</p><h2 id="条件随机场crf"><a class="anchor" href="#条件随机场crf">#</a> 条件随机场 CRF</h2><p>输入：句子<br>输出：标签序列</p><p>总结：</p><ul><li>利用神经网络进行文本特征抽取</li><li>不仅考虑标签本身，也考虑标签的转移得分（对比 softmax）</li><li>CRF 可以选择更丰富的特征</li></ul><h2 id="不规则命名实体识别方法比较"><a class="anchor" href="#不规则命名实体识别方法比较">#</a> 不规则命名实体识别方法比较</h2><p><img data-src="2.png" alt></p><h1 id="句法分析"><a class="anchor" href="#句法分析">#</a> 句法分析</h1><p>概率上下文无关句法 PCFG</p><h1 id="依存分析"><a class="anchor" href="#依存分析">#</a> 依存分析</h1><p>定义：基于依存文法的句法分析。分析结果为句子中词语间依存关系组成的依存树。</p><p>每个依存关系由一个中心词 (Head) 和一个依赖词 (Dependent) 组成</p><p>基于转换的 (transition-based) 依存句法分析：</p><ul><li>将依存分析转化为操作序列生成的问题</li><li>训练模型<ul><li>给一些训练数据（依存树库）</li><li>训练一个分类器，能够生成如下操作组成的序列<ul><li>Shift</li><li>Left arc</li><li>Right arc</li></ul></li></ul></li><li>预测时<ul><li>输入：一个句子</li><li>输出：操作序列</li><li>解码：使用操作序列将句子转化为依存树</li></ul></li></ul><h1 id="预训练"><a class="anchor" href="#预训练">#</a> 预训练</h1><p>训练指为了某个特定任务，使用人工标注的数据，训练模型（比如实体识别）</p><p>预训练也是训练模型，不过是使用大规模无人工标注的数据，进行训练</p><ul><li>标注是数据里自带的，也叫做自监督训练</li><li>可以看成无监督学习的一种</li></ul><p>word2vec 中也用过预训练</p><p>与训练模型：ELMo、GPT、BERT</p><ul><li>BERT 本质上是一个基于 Transformer 的预训练语言模型，只有 Transformer 的 Encoder</li><li>GPT 也一个基于 Transformer 的预训练语言模型，只有 Transformer 的 Decoder</li><li>ELMo 是一个基于 LSTM 的语言模型，由 forward 和 backward LSTM 组成</li></ul><h2 id="gpt和bert比较"><a class="anchor" href="#gpt和bert比较">#</a> GPT 和 BERT 比较</h2><p>GPT (Generative Pre-Training) 和 BERT (Bidirectional Encoder Representation from Transformers) 都是以 Transformer 为主题架构的预训练语言模型，都是通过 “预训练 + fine tuning” 的模式下完成下游任务的搭建</p><ul><li>GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型</li><li>GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面</li><li>同等参数规模下，BERT 的效果要好于 GPT</li></ul><h1 id="词汇处理"><a class="anchor" href="#词汇处理">#</a> 词汇处理</h1><p>词义排岐：给定一个词及其上下文，如果其义项出现在训练集中，则确定其义项</p><p>词义学习：给定一个词及其上下文，如果其义项不在训练集中，则学习其新义项，并抽取上下文特征作为该义项的标记</p><h1 id="结构分析"><a class="anchor" href="#结构分析">#</a> 结构分析</h1><p>语言结构：一个语言单位的组成部分及其关系组成的结构体<br>语言单位：短语、句子、段落、篇章<br>组成部分：字、词、短语、句子、段落、篇章</p><p>句法关系：反映句子组成层面的关系，回答句子是否合法的问题</p><p>语义关系：反映组成部分深层的关系</p><p>句法和语义：句法关心一个语言单位是否满足语法，语义关心一个语言单位所表达的意义</p><p>句法结构：语言成分及其句法关系组成的结构<br>依存结构：词汇间的依存关系组成的结构<br>块结构：句法块及其句法关系组成的结构</p><p>结构预测模型：</p><ul><li>基于转移的模型：有限自动机</li><li>基于图的模型：图空间</li></ul><h1 id="篇章分析"><a class="anchor" href="#篇章分析">#</a> 篇章分析</h1><p>回指：语法描写中用来指一个语言单位从先前某个已表达的单位或意义（先行词）得出自身释义的过程或结果</p><p>零形回指（zero anaphora）是小句中有指称前文语段的意义，但没有借助语音或者词汇形式的回指现象</p><h1 id="情感分析"><a class="anchor" href="#情感分析">#</a> 情感分析</h1><p>情感分析定义：社交媒体针对特定对象的主观反应或自身的情绪状态，前者包括极性、立场、意见、态度等；后者包括情绪表达 / 诱因抽取和分类</p><p>社交媒体文本挖掘：</p><ul><li>数据层：数据采集</li><li>资源层：知识图谱 标注数据</li><li>技术层：知识挖掘 情感分析</li><li>功能层：情报生成 预警预测</li></ul><h1 id="语义分析"><a class="anchor" href="#语义分析">#</a> 语义分析</h1><p>语义结构：自然语言所描述的语义要素及其关系</p><h1 id="实验"><a class="anchor" href="#实验">#</a> 实验</h1><p>文本分类任务基本流程：<br>安装并导入相关的深度学习库、数据获取和预处理、定义神经网络、定义损失函数 (loss function) 和优化器 (optimizer)、训练网络和测试网络</p><p>一个常规的序列标注任务代码开发流程是：安装并导入相关的深度学习库、定义标签集合 (Label set)、数据获取和预处理、定义神经网络、定义损失函数 (loss function) 和优化器 (optimizer)、训练网络和测试网络。</p><div class="tags"><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%9C%AC%E7%A7%91%E8%AF%BE%E7%A8%8B/" rel="tag"><i class="ic i-tag"></i> 计算机本科课程</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2022-01-04 11:05:58" itemprop="dateModified" datetime="2022-01-04T11:05:58+08:00">2022-01-04</time> </span><span id="Undergraduate-computer-course/nlp/review/" class="item leancloud_visitors" data-flag-title="review" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="おうじせん 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="おうじせん 支付宝"><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>おうじせん <i class="ic i-at"><em>@</em></i>OSAKANA</li><li class="link"><strong>本文链接：</strong> <a href="https://osakana373.github.io/Undergraduate-computer-course/nlp/review/" title="review">https://osakana373.github.io/Undergraduate-computer-course/nlp/review/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC96aC1DTg=="><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/Undergraduate-computer-course/public-opinion-analysis/review/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2023&#x2F;10&#x2F;17&#x2F;YiNs6V4EJfhM29q.jpg" title="review"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 舆情分析</span><h3>review</h3></a></div><div class="item right"><a href="/French/grammar/french-grammar-1-8/french-grammar-1-8/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2023&#x2F;10&#x2F;17&#x2F;YNL7WZlw1oFyfnz.jpg" title="french-grammar-1-8"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 法语语法</span><h3>french-grammar-1-8</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E8%AF%8D"><span class="toc-number">1.</span> <span class="toc-text">分词</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95"><span class="toc-number">1.1.</span> <span class="toc-text">分词算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0"><span class="toc-number">1.2.</span> <span class="toc-text">新词发现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%8D%E5%90%91%E9%87%8F"><span class="toc-number">2.</span> <span class="toc-text">词向量</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A"><span class="toc-number">2.1.</span> <span class="toc-text">名词解释</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.1.</span> <span class="toc-text">语言模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.2.</span> <span class="toc-text">神经语言模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#word2vec"><span class="toc-number">2.2.</span> <span class="toc-text">Word2Vec</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB"><span class="toc-number">3.</span> <span class="toc-text">文本分类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E4%BB%8B%E7%BB%8D"><span class="toc-number">3.1.</span> <span class="toc-text">文本分类任务介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8Ecnn%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB"><span class="toc-number">3.2.</span> <span class="toc-text">基于 CNN 的文本分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8Ecnn%E7%9A%84%E5%85%B3%E7%B3%BB%E5%88%86%E7%B1%BB"><span class="toc-number">3.3.</span> <span class="toc-text">基于 CNN 的关系分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E7%B3%BB%E5%88%86%E7%B1%BB%E7%9A%84%E4%BB%BB%E5%8A%A1%E4%BB%8B%E7%BB%8D"><span class="toc-number">3.3.1.</span> <span class="toc-text">关系分类的任务介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">3.3.2.</span> <span class="toc-text">注意力机制</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8"><span class="toc-number">4.</span> <span class="toc-text">词性标注</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E4%BB%8B%E7%BB%8D"><span class="toc-number">4.1.</span> <span class="toc-text">任务介绍</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB"><span class="toc-number">5.</span> <span class="toc-text">实体识别</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E4%BB%8B%E7%BB%8D-2"><span class="toc-number">5.1.</span> <span class="toc-text">任务介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">5.2.</span> <span class="toc-text">循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#lstm"><span class="toc-number">5.2.1.</span> <span class="toc-text">LSTM</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BAcrf"><span class="toc-number">5.3.</span> <span class="toc-text">条件随机场 CRF</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E8%A7%84%E5%88%99%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95%E6%AF%94%E8%BE%83"><span class="toc-number">5.4.</span> <span class="toc-text">不规则命名实体识别方法比较</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90"><span class="toc-number">6.</span> <span class="toc-text">句法分析</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90"><span class="toc-number">7.</span> <span class="toc-text">依存分析</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">8.</span> <span class="toc-text">预训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#gpt%E5%92%8Cbert%E6%AF%94%E8%BE%83"><span class="toc-number">8.1.</span> <span class="toc-text">GPT 和 BERT 比较</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%8D%E6%B1%87%E5%A4%84%E7%90%86"><span class="toc-number">9.</span> <span class="toc-text">词汇处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90"><span class="toc-number">10.</span> <span class="toc-text">结构分析</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AF%87%E7%AB%A0%E5%88%86%E6%9E%90"><span class="toc-number">11.</span> <span class="toc-text">篇章分析</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90"><span class="toc-number">12.</span> <span class="toc-text">情感分析</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90"><span class="toc-number">13.</span> <span class="toc-text">语义分析</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">14.</span> <span class="toc-text">实验</span></a></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/Undergraduate-computer-course/nlp/%E8%AF%8D%E5%90%91%E9%87%8F/" rel="bookmark" title="词向量">词向量</a></li><li class="active"><a href="/Undergraduate-computer-course/nlp/review/" rel="bookmark" title="review">review</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="おうじせん" data-src="/images/touxiang.jpg"><p class="name" itemprop="name">おうじせん</p><div class="description" itemprop="description">誰よりスキだから</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">25</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">14</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">4</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL29zYWthbmEzNzM=" title="https:&#x2F;&#x2F;github.com&#x2F;osakana373"><i class="ic i-github"></i></span> <span class="exturl item twitter" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS9XSFV3eng=" title="https:&#x2F;&#x2F;twitter.com&#x2F;WHUwzx"><i class="ic i-twitter"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvcGxheWxpc3Q/aWQ9Mzk5MDkyNDE2" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;playlist?id&#x3D;399092416"><i class="ic i-cloud-music"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/Undergraduate-computer-course/public-opinion-analysis/review/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/French/grammar/french-grammar-1-8/french-grammar-1-8/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/Front-EndDevelopment/" title="分类于 前端开发">前端开发</a> <i class="ic i-angle-right"></i> <a href="/categories/Front-EndDevelopment/We-chat/" title="分类于 微信小程序">微信小程序</a></div><span><a href="/Front-EndDevelopment/We-chat/rich-text%E5%AF%8C%E6%96%87%E6%9C%AC%E6%A0%87%E7%AD%BE/" title="rich-text富文本标签">rich-text富文本标签</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Undergraduate-computer-course/" title="分类于 计算机本科课程">计算机本科课程</a> <i class="ic i-angle-right"></i> <a href="/categories/Undergraduate-computer-course/cryptography/" title="分类于 密码学">密码学</a></div><span><a href="/Undergraduate-computer-course/cryptography/ModpDES/" title="ModpDES">ModpDES</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/French/" title="分类于 法语">法语</a> <i class="ic i-angle-right"></i> <a href="/categories/French/grammar/" title="分类于 法语语法">法语语法</a></div><span><a href="/French/grammar/french-grammar-21-24/french-grammar-21-24/" title="french-grammar-21-24">french-grammar-21-24</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/French/" title="分类于 法语">法语</a> <i class="ic i-angle-right"></i> <a href="/categories/French/grammar/" title="分类于 法语语法">法语语法</a></div><span><a href="/French/grammar/french-grammar-25-28/french-grammar-25-28/" title="french-grammar-25-28">french-grammar-25-28</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/French/" title="分类于 法语">法语</a> <i class="ic i-angle-right"></i> <a href="/categories/French/grammar/" title="分类于 法语语法">法语语法</a></div><span><a href="/French/grammar/french-grammar-29-32/french-grammar-29-32/" title="french-grammar-29-32">french-grammar-29-32</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/JavaScript/" title="分类于 JavaScript">JavaScript</a></div><span><a href="/Front-EndDevelopment/ES6/ES6%E6%96%B0%E7%89%B9%E6%80%A7/" title="ES6新特性">ES6新特性</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Undergraduate-computer-course/" title="分类于 计算机本科课程">计算机本科课程</a> <i class="ic i-angle-right"></i> <a href="/categories/Undergraduate-computer-course/info-hiding/" title="分类于 信息隐藏技术">信息隐藏技术</a></div><span><a href="/Undergraduate-computer-course/info-hiding/VAE-Stega-Reading-Report/" title="VAE-Stega-Reading-Report">VAE-Stega-Reading-Report</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Front-EndDevelopment/" title="分类于 前端开发">前端开发</a> <i class="ic i-angle-right"></i> <a href="/categories/Front-EndDevelopment/We-chat/" title="分类于 微信小程序">微信小程序</a></div><span><a href="/Front-EndDevelopment/We-chat/button%E6%8C%89%E9%92%AE/" title="button按钮">button按钮</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Undergraduate-computer-course/" title="分类于 计算机本科课程">计算机本科课程</a> <i class="ic i-angle-right"></i> <a href="/categories/Undergraduate-computer-course/AI/" title="分类于 人工智能">人工智能</a> <i class="ic i-angle-right"></i> <a href="/categories/Undergraduate-computer-course/AI/nlp/" title="分类于 自然语言处理">自然语言处理</a></div><span><a href="/Undergraduate-computer-course/nlp/%E8%AF%8D%E5%90%91%E9%87%8F/" title="词向量">词向量</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/French/" title="分类于 法语">法语</a> <i class="ic i-angle-right"></i> <a href="/categories/French/grammar/" title="分类于 法语语法">法语语法</a></div><span><a href="/French/grammar/french-grammar-33-36/french-grammar-33-36/" title="french-grammar-33-36">french-grammar-33-36</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2021 – <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">おうじせん @ OSAKANA</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">90k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">1:22</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"Undergraduate-computer-course/nlp/review/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->