<!-- build time:Tue Oct 17 2023 18:24:36 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="OSAKANA" href="https://osakana373.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="OSAKANA" href="https://osakana373.github.io/atom.xml"><link rel="alternate" type="application/json" title="OSAKANA" href="https://osakana373.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="计算机本科课程"><link rel="canonical" href="https://osakana373.github.io/Undergraduate-computer-course/info-hiding/VAE-Stega-Reading-Report/"><title>VAE-Stega-Reading-Report - 信息隐藏技术 - 计算机本科课程 | OSAKANA = OSAKANA = 一緒に夢を見よう</title><meta name="generator" content="Hexo 5.4.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">VAE-Stega-Reading-Report</h1><div class="meta"><span class="item" title="创建时间：2021-12-23 20:00:49"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2021-12-23T20:00:49+08:00">2021-12-23</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>6.9k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>6 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">OSAKANA</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://s2.loli.net/2023/10/17/M9zNVT67rhDRkpI.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2023/10/17/NaQTivZ1XtCLSEd.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2023/10/17/6VYTuKLQGMydihI.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2023/10/17/kdxv6E3sgahQRlj.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2023/10/17/mDQ6o8sgVwOcCPS.jpg"></li><li class="item" data-background-image="https://s2.loli.net/2023/10/17/YNL7WZlw1oFyfnz.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/Undergraduate-computer-course/" itemprop="item" rel="index" title="分类于 计算机本科课程"><span itemprop="name">计算机本科课程</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/Undergraduate-computer-course/info-hiding/" itemprop="item" rel="index" title="分类于 信息隐藏技术"><span itemprop="name">信息隐藏技术</span></a><meta itemprop="position" content="2"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://osakana373.github.io/Undergraduate-computer-course/info-hiding/VAE-Stega-Reading-Report/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/touxiang.jpg"><meta itemprop="name" content="おうじせん"><meta itemprop="description" content="一緒に夢を見よう, 誰よりスキだから"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="OSAKANA"></span><div class="body md" itemprop="articleBody"><h1 id="vae-stega基于变分自编码器的语言隐写技术"><a class="anchor" href="#vae-stega基于变分自编码器的语言隐写技术">#</a> VAE-Stega：基于变分自编码器的语言隐写技术</h1><h2 id="摘要"><a class="anchor" href="#摘要">#</a> 摘要</h2><p>近年来，基于文本自动生成技术的语言隐写技术得到了极大的发展，这是一个非常有前途的研究课题，但也是一个非常具有挑战性的研究课题。以往的工作主要集中在优化语言模型和条件概率编码方法，旨在生成质量更好的隐写句子。本文中报道了一些最新的实验结果，这似乎表明生成的隐写文本的质量不能完全保证其隐写文本的安全性，甚至具有显著的感知不可感知和统计不可感知冲突效应 (Psic 效应)。为了进一步提高生成的隐写文本的不可感知性和安全性，本文提出了一种新的基于变分自编码器 (VAE) 的语言隐写技术，可称为 VAE-Stega。我们使用 VAE-Stega 编码器学习大量正常文本的整体统计分布特征，然后使用解码器 VAE-Stega 生成隐写句子符合统计语言模型以及整体正常句子的统计分布，以保证生成的隐写的不可感知和统计不可感知文本。我们设计了几个实验来验证所提出的方法。实验结果表明，该模型可以大大提高生成的隐写句子的不可感知性，从而达到了最先进的性能。</p><h2 id="介绍"><a class="anchor" href="#介绍">#</a> 介绍</h2><p>从理论上讲，任何具有冗余信息空间的载体都可以用来隐藏内部的秘密信息，例如图像、视频、文本等。</p><p>Fridrich 总结说，一般隐写算法可分为三类：载体选择、载体修改和载体生成。基于载体生成的隐写术与其他两种方法的最大区别是：另外两种隐写方法将得到一个具有完全语义的载体，然后将秘密信息隐藏在里面。然而，基于载体生成的隐写方法需要自己生成载体。</p><p>两大挑战：</p><ol><li>如何自动生成一个语义完整、足够自然的信息载体</li><li>即使可以解决第一个挑战，如何进一步保证这些生成的隐写载体<br>的不可感知性</li></ol><p>本文将从理论分析、模型设计和实验结果等角度来回答这些问题。首先遵循 Cachin 对隐写算法安全性的分析，指出文本隐写的核心目标是尽可能减少正态文本和隐写文本之间的统计分布差异，如下图所示。</p><p><img data-src="fig1.jpg" alt="为了保证生成的隐写句子的不可感知性，隐写方法应尽量减少正常句子和隐写句子之间的总体统计分布差异"></p><p>其次，我们报告了我们最新的实验结果，这些结果表明，生成的隐写文本的质量并不完全等同于它们的不可感知性。为了进一步阐明<br>这两个概念之间的区别，我们将文本的不可感知性分别分为感知 - 不可感知性和统计 - 不可感知性两部分。第三，为了进一步提高生成的隐写文本的不可感知性，本文提出了一种基于变分自编码器 (VAE) 的语言隐写技术，可称为 VAE-Stega。利用 VAE-Stega 中的编码器学习正常文本的总体统计分布特征，然后利用 VAE-Stega 中的解码器生成符合统计语言模型和总体统计分布的隐写句子，从而保证生成的隐写文本的感知不可感知性和统计不可感知性，从而提高整个秘密通信处理的安全性。</p><h2 id="相关工作"><a class="anchor" href="#相关工作">#</a> 相关工作</h2><p>目前，大多数的隐写文本自动生成模型在以下框架下：使用一个精心设计的模型学习统计语言模型从大量的正常句子，然后实现秘密信息隐藏通过编码的条件每个词的概率分布在文本生成过程。在此框架中，早期的工作主要使用马尔可夫模型来近似语言模型，并计算每个单词、的条件概率分布。然而，由于马尔可夫模型本身的局限性，马尔可夫模型生成的文本质量仍然不够好，因此很容易被识别。近年来，随着自然语言处理技术的发展，越来越多的基于神经网络模型的隐写文本生成模型出现了。首先将字典划分，对每个单词进行固定编码，然后使用递归神经网络 (RNN) 学习自然文本的统计语言模型。最后，在文本生成过程中，根据需要隐藏的信息在每一步选择不同的单词作为输出。本文中将编码过程改为基于条件概率分布的动态编码，提出使用完整的二叉树或霍夫曼树对每个单词的条件概率进行固定长编码 (FLC) 或变长编码 (VLC)，并根据需要隐藏的秘密信息输出相应的单词，实现隐藏信息在句子生成过程中的嵌入。随后进一步提出了一种改进的编码算法称为 patient-Huffman，利用每个单词的条件概率分布的差异 (KL) 生成过程中动态调整每个单词的嵌入率，进一步提高生成的隐写文本的质量。</p><p>这些基于神经网络的文本自动生成方法比马尔可夫模型能更好地拟合正常句子的统计语言模型，从而显著提高了生成的隐写文本的质量。但最新的实验结果表明，这些方法只能解决第一个挑战，即自动生成语义完整和足够自然的隐写句子。</p><h2 id="psic效应"><a class="anchor" href="#psic效应">#</a> Psic 效应</h2><p>以往基于神经网络的隐写文本生成模型旨在尽可能多地生成具有最优条件概率的单词序列。这一优化目标只能保证生成的高质量的隐写文本。但是生成的隐写句子的质量越好，它们的不可感知性是否优秀存疑，因此本文做了一些实验来验证这个猜想。</p><p>首先，我们在 IMDB 数据集上训练 RNN-Stega 模型，然后使用训练后的 RNN-Stega 模型在每个不同的嵌入速率下生成 1000 个隐写句子。其次，我们计算了这些生成的隐写句子在不同嵌入率下的平均复杂度。在自然语言处理领域，困惑度 (perplexity) 是语言模型测试的标准度量。通常，困惑度值越小，生成的句子的语言模型越好。第三，我们混合了不同嵌入率的句子，然后我们严格遵循双盲实验的要求并邀请 11 名经过英语表达和阅读能力筛选的人对这些句子的质量进行评分（1-5 分，越高越好）。我们计算了这些生成的隐写句子在不同嵌入率下的平均人类得分，这可以代表这些生成的句子的主观质量。最后，我们将不同嵌入率的隐写句子与相同数量的正常句子混合，然后使用最近提出的文本隐分析模型进行检测。我们重复了这个实验 10 次，记录不同嵌入率下的检测精度的平均值和标准偏差。实验结果 1 如下图所示，其中横坐标表示每个单词中嵌入的比特的平均数量 (bpw)。橙色的线表示这些生成的隐写句子中计算出的平均困惑度（越小越好）。点的大小和透明度代表了人类的平均得分（越大越好）。蓝线表示不同嵌入率下的隐写检测精度（越低越好）。</p><p><img data-src="fig2.jpg" alt="RNN-Stega所面临的Psic效应。可以看出，随着嵌入率的提高，RNN-Stega生成的隐写文本质量逐渐下降，但其反步分析能力逐渐提高。我们将这种感知-不可感知和统计-不可感知的冲突现象称为感知-统计-不可感知冲突效应(Psic效应)"></p><p>根据上图，我们可以得出以下结论。首先，随着嵌入率的增加，生成的隐写句子的困惑度趋势和人的得分是一致的，它们都表明生成的隐写句子的质量在下降。同时，我们发现生成的隐写句子的质量并不等同于它们的不可感知性。当 bpw 小于 2 时生成的隐写句子质量最好，但最容易发现；当 bpw 在 5 左右时，生成的隐写句子质量最差，但其抵抗隐分析的能力最强。第三，我们注意到一个非常不寻常的现象：随着嵌入率的增加，隐写检测的准确性逐渐降低，这意味着生成的隐写句子更加难以检测。</p><p>在最近的一个预印本中也报告了类似的实验结果。实验表明，随着嵌入率的增加，生成的隐写句子与正常句子的条件概率分布的 KL 散度逐渐减小（越低越好），但人类评价得分也逐渐减小。这种现象似乎与我们通常的认知方式有所不同。一般来说，我们认为随着嵌入在载体中的附加信息量的增加，载体的不可感知性将会降低，这将反映在隐写分析精度的提高上。为了进一步解释这一异常现象，我们进行了更深入的实验分析。我们计算了每个生成的隐写句子在不同嵌入率下的困惑度，并进行了比较与正常句子的分布情况相同。结果如下图所示：</p><p><img data-src="fig3.jpg" alt="黑线表示正常句子的分布，其他颜色的线表示由RNN-Stega模型生成的具有不同嵌入率的隐写文本"></p><p>上图可以很好地解释小节 Pisc 小节中的现象。公共社交网络上的文本是由不同年龄和背景的人以不同的表达方式编写的句子。这就导致了大多数人类所写的事实句子可能不服从最优语言模型，形成较大的方差。之前的自动文本生成隐写模型主要侧重于优化每个单词的条件概率分布的语言模型和编码方法，主要解决第一个挑战。</p><p>随着嵌入率的提高，生成的隐写载体的质量和反隐写分析能力的不同趋势代表了这些生成的隐写载体的两个不同方面，我们将它们命名为感知 - 不可感知性和统计 - 不可感知性，如下图所示。感知不可感知性衡量的是生成的隐写载体的质量，而统计不可感知性衡量的是生成的隐写载体与正常载体之间的统计不可分性。</p><p><img data-src="fig4.jpg" alt="生成的隐写载体的不可感知性由两部分组成：感知不可感知性，表示每个单独生成的载体的质量，以及统计不可感知性，表示生成的载体与普通载体在统计上不可分割"></p><h2 id="ts-vae-methodology"><a class="anchor" href="#ts-vae-methodology">#</a> TS-VAE METHODOLOGY</h2><h3 id="变分自动编码器的结构"><a class="anchor" href="#变分自动编码器的结构">#</a> 变分自动编码器的结构</h3><p>一个变分的自动编码器符合一般的自动编码器架构，它通常是映射的单个样本 x 通过编码器到一个特征空间 z∈Z，然后使用解码器将该特征重构为一个原始样本，即：</p><p><img data-src="fig4.png" alt></p><p>然后，通过最小化重建的样本 x' 与原始样本 x 之间的差值，我们可以得到从样本空间到特征空间的最优映射。然而，自动编码器只学习样本与特征之间的点对点映射，因此解码器只能重建原始样本，而不能生成新的样本。由于统计不可感知性要求生成的隐写句子与正态句子的统计分布足够接近，因此我们的基本思想是找到一个编码函数，将正态句子嵌入到一个潜在空间 Z 中，形成 Pz 分布。然后，我们可以根据潜在空间的分布进行随机采样，得到一个采样的潜在向量 z。我们将这个向量 z 发送到解码器中，然后在 z 的约束下生成一个句子，从而保持生成的句子的分布与正常句子的分布一致。</p><p><img data-src="fig5.png" alt><br><img data-src="fig6.png" alt></p><h3 id="encoder-in-vae-stega"><a class="anchor" href="#encoder-in-vae-stega">#</a> Encoder in VAE-Stega</h3><p>在 VAE 架构中，任何模型都可以作为编码器使用，只要它可以提取输入句子的特征表达式并将其映射到特征空间 Z。在本文中，我们设计并比较了两种不同的编码器，其中一种他们使用一个以 LSTM 单元作为编码器的递归神经网络，我们称之为 VAE-Stega (LSTM-LSTM)。另一种是使用来自变压器 (BERT) 的双向编码器表示作为编码器，我们称之为 VAE-Stega (bertlstm)。</p><p>VAEStega (bertlstm) 结构如下图所示：</p><p><img data-src="fig7.jpg" alt="我们使用VAE-Stega编码器学习大量的正常句子的统计分布特征，然后使用解码器VAE-Stega生成隐写句子符合统计语言模型以及正常句子的总体统计分布，以保证生成的感知不可感知和统计不可感知文本"></p><h3 id="decoder-in-vae-stega"><a class="anchor" href="#decoder-in-vae-stega">#</a> Decoder in VAE-Stega</h3><p>RNN 可以从大量的正常文本中学习统计语言模型，然后根据之前生成的单词计算下一个单词的条件概率分布，最后可以生成符合该统计语言模型的句子。</p><p>对于 VAE-Stega (LSTM-LSTM)，它与 VAEStega (bertlstm) 共享相同的模型结构和相同的解码器，唯一的区别是编码器模块。VAE-Stega (LSTM-LSTM) 使用了一个以 LSTM 单元为编码器的递归神经网络。它使用最后一个时间步中最后一个隐藏层的输出作为输入句子的特征表达式。因此，对于 VAE-Stega (LSTM-LSTM)，我们只需要用以下形式替换公式：</p><p><img data-src="fig8.png" alt></p><p>其中，l 表示 VAE-Stega (LSTM-LSTM) 编码器中的隐藏层数。提取输入句子的特征表达式后，后续操作与上述 VAEStega (BERT-LSTM) 完全一致。</p><h3 id="信息的隐藏和提取"><a class="anchor" href="#信息的隐藏和提取">#</a> 信息的隐藏和提取</h3><p>与之前的隐写文本生成算法，如 RNN-Stega 模型相比，所提出的 VAE-Stega 模型在译码每个隐写句子的过程中，需要使用从潜在空间采样的相同的潜在向量。在实际使用中，由于计算机只能生成伪随机数，发送方和接收方只需要共享随机种子生成算法，以确保采样的潜在向量同步，从而确保在接收到的隐写文本中正确提取秘密信息。在每个时间步长中，Bob 将每个单词输入到相同的训练模型中，并得到下一个单词的条件概率分布。他首先将字典中的所有单词按概率降序排序，并选择顶部的 m 个单词形成候选池。然后，他使用与发送者相同的编码方法对候选池进行编码，如霍夫曼编码或算术编码。最后，根据当前时刻的实际传输字，接收器能够成功、准确地解码每个字中嵌入的位，从而完成隐蔽通信。</p><h2 id="实验和分析"><a class="anchor" href="#实验和分析">#</a> 实验和分析</h2><p>在本节中，我们进行了几个实验来测试所提出的 VAE-Stega 模型的性能。首先，我们介绍了我们在实验中使用的数据集，以及模型设置和训练细节。然后，我们主要分析了该模型生成的隐写句子的不可感知性。</p><h3 id="数据准备和模型训练"><a class="anchor" href="#数据准备和模型训练">#</a> 数据准备和模型训练</h3><p>在这项工作中，我们使用了两个大规模的公共文本数据集来训练我们的模型，它们分别是推特和 IMDB 电影评论。预处理后，包括将所有单词转换成小写，删除特殊符号，表情符号，web 链接和过滤低频单词，推特数据集包含 2639083 正常句子字典大小为 44856，IMDB 数据集包含总共 1282804 正常句子字典大小为 48042。VAE-Stega 模型中几乎所有的参数都可以通过训练得到，但仍有一些超参数需要确定。</p><p>通过多次比较实验，将这些超参数最终设置如下。对于解码器，我们使用与 RNN-Stega 相同的设置来生成句子，即我们将 LSTM 隐藏层数设置为 3 个，每一层包含 800 个 LSTM 单位。我们使用了 BERTbase 作为 VAE-Stega (BERT-LSTM) 中的编码器，其中包含 12 个 Transformer 块，隐藏大小 768，自注意头 12 个。我们将潜在空间的维数 (dimension of latent space) 设为 13，高速公路层数 (Highway layers) 为 2，隐藏维数为 1600。对于 VAE-Stega (LSTM-LSTM) 模型中的编码器，我们将其设置为与解码器相同。字典中的每个单词都被映射到一个 353 维的向量，我们使用 tanh 作为非线性激活函数。</p><p>神经网络的所有参数都需要通过训练来获得。在训练过程中，我们使用反向传播算法更新网络参数。通过最小化损失函数，我们希望所提出的模型能够同时学习正常句子的统计语言模型及其整体分布模式。在模型训练过程中，我们采用了 KL 代价退火策略，使模型能够更好地考虑语言模型和总体统计分布约束，使得解码器生成的隐写句子具有较高的感知不可感知性和统计不可感知性。</p><p><img data-src="fig9.jpg" alt="在模型训练过程中，损失树随着迭代步骤的变化而变化"></p><h3 id="不可感知分析"><a class="anchor" href="#不可感知分析">#</a> 不可感知分析</h3><p>为了测量所提出的 VAE-Stega 模型生成的隐写句子的不可感知性，根据之前的分析，我们需要对生成的隐写句子的语言模型进行测试，以困惑度 (ppl) 来衡量；另一方面，我们需要测试生成的隐写句子与正常句子的总体分布的差异，后者用 KLD 和 JSD 来测量。</p><p><img data-src="fig10.png" alt></p><p>具体测试表格见论文表一表二。这里仅说明结论。</p><p>根据表一所示的结果，我们可以得出以下结论。首先，我们在第三节 Psic 效应分析，之前的模型的优化目标，如 RNN-Stega，是确保生成的条件概率的每个单词的句子尽可能高，从而使生成隐写句子的质量尽可能好。最终结果表明，所生成的隐写句子具有较低的困惑度值。但如 MP、KLD 和 JSD 的计算结果所示，它们与正常句子的总体统计分布存在巨大差异。其次，我们发现，在文本生成过程中，对条件概率分布使用算术编码可以减少在单词的条件概率分布与霍夫曼编码的比较中生成的隐写文本和普通文本之间的差异。然而，通过比较 RNN-Stega (HC) 和 RNN-Stega (AC) 的 KLD 和 JSD，因为它们对文本的总体统计分布缺乏约束，这两种编码方法生成的隐写文本与正常文本的总体统计分布仍有很大的差异（算术编码略优于霍夫曼编码）。第三，我们发现，我们引入 VAE 架构后，模型因此可以学习正常文本的整体统计分布特征和进一步约束生成的隐写文本在一定程度上，这可以显著减少生成的隐写句子和正常的句子的总体统计分布的差异，极大地提高了统计的不可感知性，在稍微失去困惑的情况下（不大，它不会显著影响生成的隐写句子的感知不可感知性）。</p><h3 id="抗隐写分析能力"><a class="anchor" href="#抗隐写分析能力">#</a> 抗隐写分析能力</h3><p>我们使用最新的隐写分析模型检测 VAE-Stega 模型和 RNN-Stega<br>模型在不同条件概率编码方法 (霍夫曼编码 (表 1) 和算术编码 (表 2) 下生成的隐写句子。我们使用在分类任务中常用的几个评估指标来评估我们的模型的性能，即准确率 (Acc) 和召回率 (R)。</p><p><img data-src="fig11.png" alt></p><p>TP 表示被模型预测为正的正样本的数量，FP 表示预测为正的负样本数量，FN 表示预测为负的正样本数量，TN 表示预测为负的负样本数量。</p><p><img data-src="table1.jpg" alt="由每个模型（使用隐霍夫曼编码）产生的隐写句子的抵抗能力"><br><img data-src="table2.jpg" alt="由每个模型（使用算术编码）产生的隐写句子的抵抗能力"></p><p>从表一表二结果中，我们可以得出以下结论。</p><ol><li>首先，虽然提出的 VAE-Stega 模型和 RNN-Stega 模型在隐写句子生成、条件概率编码和秘密信息隐藏等方面是一致的，但我们在隐写句子生成过程中考虑了正常句子的总体统计分布特征，大大提高了 VAE-Stega 生成的隐写句子的统计不可感知性，也大大提高了其抗隐写分析的能力。</li><li>其次，表一表二的结果再次证实了 Psic 效应，即随着嵌入率的提高，不同模型在不同数据集中生成的隐写分析文本的检测精度逐渐降低。然而，值得注意的是，我们认为这是当前隐写文本生成方法框架的独特现象：使用一个设计良好的模型来从大量的正常句子学习统计语言模型，然后实现秘密信息隐藏通过编码的条件概率分布的文本生成过程。这种方法在生成每个单词时截断了它们的条件概率。随着嵌入率的增加，截断的条件概率分布逐渐接近真实分布，因此生成的隐写文本中每个单词的条件概率分布的差逐渐接近正常句子的差。虽然这种现象可能很特殊，但我们认为它应该被视为这种隐写策略的一个特征，而不是一种缺点。毕竟，在实际使用方面，我们确实希望当嵌入率相对较大时，我们仍然能保持相对较小的隐写检测精度。</li><li>此外，需要注意的是，表一、表二中的一些结果似乎表明，VAEStega (bertlstm) 的性能比 VAE-Stega (LSTM-LSTM) 更差。这并不意味着我们认为 BERT 的特征提取能力低于 LSTM。实际上，这是因为我们直接使用了已经发布的训练过的 BERT 模型。它已经对我们的其他数据集进行了预训练，这可能会影响其性能。但是 VAE-Stega (LSTM-LSTM) 是从头开始训练的，所以 VAE-Stega (LSTM-LSTM) 的编码器和解码器使用相同的单词嵌入空间。</li></ol><h2 id="结论"><a class="anchor" href="#结论">#</a> 结论</h2><p>两大挑战：</p><ol><li>如何自动生成一个语义完整、足够自然的信息载体</li><li>即使可以解决第一个挑战，如何进一步保证这些生成的隐写载体的不可感知性</li></ol><p>解决第二个挑战，本文提出了一种新的隐写法 VAE-Stega。我们使用编码器在 VAE-Stega 学习整体统计分布特征的大量正常的文本，然后使用解码器 VAE-Stega 生成隐写句子符合统计语言模型以及正常句子的整体统计分布，以平衡和优化两个方面的不可感知性的 Psic 效应的影响。实验结果表明，与以往的方法相比，该模型能大大提高生成的隐写句子的不可感知性。</p><div class="tags"><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%9C%AC%E7%A7%91%E8%AF%BE%E7%A8%8B/" rel="tag"><i class="ic i-tag"></i> 计算机本科课程</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2021-12-23 22:54:32" itemprop="dateModified" datetime="2021-12-23T22:54:32+08:00">2021-12-23</time> </span><span id="Undergraduate-computer-course/info-hiding/VAE-Stega-Reading-Report/" class="item leancloud_visitors" data-flag-title="VAE-Stega-Reading-Report" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="おうじせん 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.png" alt="おうじせん 支付宝"><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>おうじせん <i class="ic i-at"><em>@</em></i>OSAKANA</li><li class="link"><strong>本文链接：</strong> <a href="https://osakana373.github.io/Undergraduate-computer-course/info-hiding/VAE-Stega-Reading-Report/" title="VAE-Stega-Reading-Report">https://osakana373.github.io/Undergraduate-computer-course/info-hiding/VAE-Stega-Reading-Report/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC96aC1DTg=="><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/Undergraduate-computer-course/cryptography/ModpDES/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2023&#x2F;10&#x2F;17&#x2F;o4t5VECPQDBbzOS.jpg" title="ModpDES"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 密码学</span><h3>ModpDES</h3></a></div><div class="item right"><a href="/Undergraduate-computer-course/public-opinion-analysis/review/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;s2.loli.net&#x2F;2023&#x2F;10&#x2F;17&#x2F;M9zNVT67rhDRkpI.jpg" title="review"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 舆情分析</span><h3>review</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#vae-stega%E5%9F%BA%E4%BA%8E%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E7%9A%84%E8%AF%AD%E8%A8%80%E9%9A%90%E5%86%99%E6%8A%80%E6%9C%AF"><span class="toc-number">1.</span> <span class="toc-text">VAE-Stega：基于变分自编码器的语言隐写技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.2.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.3.</span> <span class="toc-text">相关工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#psic%E6%95%88%E5%BA%94"><span class="toc-number">1.4.</span> <span class="toc-text">Psic 效应</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ts-vae-methodology"><span class="toc-number">1.5.</span> <span class="toc-text">TS-VAE METHODOLOGY</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%98%E5%88%86%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8%E7%9A%84%E7%BB%93%E6%9E%84"><span class="toc-number">1.5.1.</span> <span class="toc-text">变分自动编码器的结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#encoder-in-vae-stega"><span class="toc-number">1.5.2.</span> <span class="toc-text">Encoder in VAE-Stega</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#decoder-in-vae-stega"><span class="toc-number">1.5.3.</span> <span class="toc-text">Decoder in VAE-Stega</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E7%9A%84%E9%9A%90%E8%97%8F%E5%92%8C%E6%8F%90%E5%8F%96"><span class="toc-number">1.5.4.</span> <span class="toc-text">信息的隐藏和提取</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E5%92%8C%E5%88%86%E6%9E%90"><span class="toc-number">1.6.</span> <span class="toc-text">实验和分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%E5%92%8C%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">1.6.1.</span> <span class="toc-text">数据准备和模型训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%8F%AF%E6%84%9F%E7%9F%A5%E5%88%86%E6%9E%90"><span class="toc-number">1.6.2.</span> <span class="toc-text">不可感知分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%97%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90%E8%83%BD%E5%8A%9B"><span class="toc-number">1.6.3.</span> <span class="toc-text">抗隐写分析能力</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">1.7.</span> <span class="toc-text">结论</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li class="active"><a href="/Undergraduate-computer-course/info-hiding/VAE-Stega-Reading-Report/" rel="bookmark" title="VAE-Stega-Reading-Report">VAE-Stega-Reading-Report</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="おうじせん" data-src="/images/touxiang.jpg"><p class="name" itemprop="name">おうじせん</p><div class="description" itemprop="description">誰よりスキだから</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">22</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">14</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">4</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL29zYWthbmEzNzM=" title="https:&#x2F;&#x2F;github.com&#x2F;osakana373"><i class="ic i-github"></i></span> <span class="exturl item twitter" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS9XSFV3eng=" title="https:&#x2F;&#x2F;twitter.com&#x2F;WHUwzx"><i class="ic i-twitter"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvcGxheWxpc3Q/aWQ9Mzk5MDkyNDE2" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;playlist?id&#x3D;399092416"><i class="ic i-cloud-music"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/Undergraduate-computer-course/cryptography/ModpDES/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/Undergraduate-computer-course/public-opinion-analysis/review/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/Undergraduate-computer-course/" title="分类于 计算机本科课程">计算机本科课程</a> <i class="ic i-angle-right"></i> <a href="/categories/Undergraduate-computer-course/AI/" title="分类于 人工智能">人工智能</a> <i class="ic i-angle-right"></i> <a href="/categories/Undergraduate-computer-course/AI/nlp/" title="分类于 自然语言处理">自然语言处理</a></div><span><a href="/Undergraduate-computer-course/nlp/review/" title="review">review</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Undergraduate-computer-course/" title="分类于 计算机本科课程">计算机本科课程</a> <i class="ic i-angle-right"></i> <a href="/categories/Undergraduate-computer-course/OS/" title="分类于 操作系统">操作系统</a> <i class="ic i-angle-right"></i> <a href="/categories/Undergraduate-computer-course/OS/Oranges/" title="分类于 Oranges">Oranges</a></div><span><a href="/Undergraduate-computer-course/OS/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%8F%AF%E6%89%A7%E8%A1%8C%E6%96%87%E4%BB%B6%E7%AF%A1%E6%94%B9/" title="操作系统之可执行文件篡改">操作系统之可执行文件篡改</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94/" title="分类于 生活随笔">生活随笔</a></div><span><a href="/2021%E5%B9%B411%E6%9C%8829%E6%97%A5/" title="2021年11月29日">2021年11月29日</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/French/" title="分类于 法语">法语</a> <i class="ic i-angle-right"></i> <a href="/categories/French/grammar/" title="分类于 法语语法">法语语法</a></div><span><a href="/French/grammar/french-grammar-1-8/french-grammar-1-8/" title="french-grammar-1-8">french-grammar-1-8</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Undergraduate-computer-course/" title="分类于 计算机本科课程">计算机本科课程</a> <i class="ic i-angle-right"></i> <a href="/categories/Undergraduate-computer-course/AI/" title="分类于 人工智能">人工智能</a> <i class="ic i-angle-right"></i> <a href="/categories/Undergraduate-computer-course/AI/public-opinion-analysis/" title="分类于 舆情分析">舆情分析</a></div><span><a href="/Undergraduate-computer-course/public-opinion-analysis/review/" title="review">review</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/French/" title="分类于 法语">法语</a> <i class="ic i-angle-right"></i> <a href="/categories/French/grammar/" title="分类于 法语语法">法语语法</a></div><span><a href="/French/grammar/french-grammar-25-28/french-grammar-25-28/" title="french-grammar-25-28">french-grammar-25-28</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/French/" title="分类于 法语">法语</a> <i class="ic i-angle-right"></i> <a href="/categories/French/grammar/" title="分类于 法语语法">法语语法</a></div><span><a href="/French/grammar/french-grammar-17-20/french-grammar-17-20/" title="french-grammar-17-20">french-grammar-17-20</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Front-EndDevelopment/" title="分类于 前端开发">前端开发</a> <i class="ic i-angle-right"></i> <a href="/categories/Front-EndDevelopment/We-chat/" title="分类于 微信小程序">微信小程序</a></div><span><a href="/Front-EndDevelopment/We-chat/%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BB%84%E4%BB%B6/" title="自定义组件">自定义组件</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Front-EndDevelopment/" title="分类于 前端开发">前端开发</a> <i class="ic i-angle-right"></i> <a href="/categories/Front-EndDevelopment/We-chat/" title="分类于 微信小程序">微信小程序</a></div><span><a href="/Front-EndDevelopment/We-chat/rich-text%E5%AF%8C%E6%96%87%E6%9C%AC%E6%A0%87%E7%AD%BE/" title="rich-text富文本标签">rich-text富文本标签</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Front-EndDevelopment/" title="分类于 前端开发">前端开发</a> <i class="ic i-angle-right"></i> <a href="/categories/Front-EndDevelopment/We-chat/" title="分类于 微信小程序">微信小程序</a></div><span><a href="/Front-EndDevelopment/We-chat/%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/" title="生命周期">生命周期</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2021 – <span itemprop="copyrightYear">2023</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">おうじせん @ OSAKANA</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">70k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">1:04</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"Undergraduate-computer-course/info-hiding/VAE-Stega-Reading-Report/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->